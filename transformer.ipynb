{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5580, 0.4346, 0.6972, 0.3066],\n",
      "        [0.2133, 0.8577, 0.0920, 0.9061],\n",
      "        [0.8073, 0.1437, 0.0165, 0.9705],\n",
      "        [0.0129, 0.6599, 0.1882, 0.1539]])\n"
     ]
    }
   ],
   "source": [
    "random_torch = torch.rand(4, 4)\n",
    "print(random_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.19 (main, May  6 2024, 20:12:36) [MSC v.1916 64 bit (AMD64)]\n",
      "d:\\ProgramData\\anaconda3\\envs\\didl\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "# 将输入的词汇表索引 转换为指定维度的Embedding\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model) -> None:\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device) -> None:\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = torch.float().unsqueeze(dim=1)\n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEmbedding(d_model, max_len, device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        return self.drop_out(tok_emb + pos_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_combine = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch, time, dimension = q.shape\n",
    "        n_d = self.d_model // self.n_head\n",
    "\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "\n",
    "        # 将dimension差分为n_head和n_d乘积 并更改time和n_head维度位置(batch,n_head,time,n_d)\n",
    "        q = q.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "\n",
    "        # k的转置的最后两维\n",
    "        score = q @ k.transpose(2, 3) / math.sqrt(n_d)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = torch.tril(torch.ones(time, time, dtype=bool))\n",
    "            score = score.masked_fill(mask == 0, -10000)  # float('-inf')\n",
    "\n",
    "        score = self.softmax(score) @ v\n",
    "\n",
    "        # 恢复原来的形状\n",
    "        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)\n",
    "        output = self.w_combine(score)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64, 512])\n",
      "tensor([[[ 0.0751, -0.0150,  0.0540,  ..., -0.0519,  0.0027,  0.0660],\n",
      "         [ 0.1109, -0.0110,  0.0643,  ..., -0.0395, -0.0152,  0.0795],\n",
      "         [ 0.0807,  0.0110,  0.0320,  ..., -0.0340, -0.0156,  0.0649],\n",
      "         ...,\n",
      "         [ 0.0918, -0.0204,  0.0507,  ..., -0.0828,  0.0026,  0.0800],\n",
      "         [ 0.0912, -0.0044,  0.0434,  ..., -0.0449, -0.0045,  0.0481],\n",
      "         [ 0.1021, -0.0036,  0.0480,  ..., -0.0416, -0.0109,  0.0660]],\n",
      "\n",
      "        [[ 0.0714, -0.0472,  0.0120,  ...,  0.0563, -0.0239,  0.0662],\n",
      "         [ 0.0714, -0.0579, -0.0357,  ...,  0.0893, -0.0078,  0.0600],\n",
      "         [ 0.0994, -0.0387, -0.0058,  ...,  0.0652, -0.0308,  0.0486],\n",
      "         ...,\n",
      "         [ 0.0659, -0.0595, -0.0206,  ...,  0.0566, -0.0110,  0.0203],\n",
      "         [ 0.0929, -0.0305, -0.0274,  ...,  0.0717, -0.0372,  0.0535],\n",
      "         [ 0.0776, -0.0703,  0.0142,  ...,  0.0877, -0.0025,  0.0265]],\n",
      "\n",
      "        [[-0.0139,  0.0194,  0.0465,  ...,  0.0348, -0.0694,  0.0988],\n",
      "         [-0.0285,  0.0096,  0.0359,  ...,  0.0250, -0.1044,  0.0983],\n",
      "         [-0.0425,  0.0260,  0.0531,  ...,  0.0173, -0.0977,  0.0611],\n",
      "         ...,\n",
      "         [-0.0609,  0.0023,  0.0354,  ...,  0.0592, -0.0759,  0.0510],\n",
      "         [-0.0410,  0.0159,  0.0174,  ...,  0.0236, -0.0832,  0.0635],\n",
      "         [-0.0303,  0.0207,  0.0298,  ...,  0.0254, -0.0779,  0.0902]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0206, -0.0835, -0.0539,  ...,  0.0592, -0.0827,  0.0681],\n",
      "         [-0.0082, -0.0387, -0.0114,  ...,  0.0479, -0.1005,  0.0833],\n",
      "         [-0.0154, -0.0299, -0.0473,  ...,  0.0695, -0.0797,  0.0907],\n",
      "         ...,\n",
      "         [ 0.0071, -0.0522, -0.0199,  ...,  0.0441, -0.0772,  0.1074],\n",
      "         [-0.0045, -0.0131, -0.0488,  ...,  0.0504, -0.0864,  0.0887],\n",
      "         [-0.0038, -0.0437, -0.0288,  ...,  0.0665, -0.0763,  0.0665]],\n",
      "\n",
      "        [[ 0.1206,  0.0095, -0.0023,  ..., -0.0152, -0.1217, -0.0141],\n",
      "         [ 0.0972,  0.0014,  0.0109,  ..., -0.0094, -0.1150, -0.0198],\n",
      "         [ 0.1035,  0.0035,  0.0072,  ..., -0.0175, -0.1270, -0.0248],\n",
      "         ...,\n",
      "         [ 0.0962, -0.0131, -0.0006,  ...,  0.0061, -0.1347, -0.0484],\n",
      "         [ 0.0901, -0.0094,  0.0076,  ..., -0.0051, -0.1381, -0.0129],\n",
      "         [ 0.1129, -0.0074,  0.0229,  ..., -0.0137, -0.1308, -0.0198]],\n",
      "\n",
      "        [[ 0.0291,  0.0477, -0.0384,  ..., -0.0917, -0.0355,  0.1309],\n",
      "         [ 0.0559,  0.0616, -0.0598,  ..., -0.0440, -0.0268,  0.1045],\n",
      "         [ 0.0378,  0.0323, -0.0547,  ..., -0.0433, -0.0571,  0.0971],\n",
      "         ...,\n",
      "         [ 0.0348,  0.0765, -0.0317,  ..., -0.0594, -0.0265,  0.1334],\n",
      "         [ 0.0500,  0.0610, -0.0579,  ..., -0.0616, -0.0362,  0.1194],\n",
      "         [ 0.0638,  0.0261, -0.0416,  ..., -0.0471, -0.0078,  0.0940]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(128, 64, 512)  # batch, time, dimension\n",
    "print(x.shape)\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "attention = MultiHeadAttention(d_model, n_head)\n",
    "out = attention(x, x, x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义softmax函数\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "# 示例\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "print(softmax(x))  # 输出：array([0.65900345, 0.24243297, 0.09856468])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "didl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
